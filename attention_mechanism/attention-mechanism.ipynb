{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "1. In the past few years, the Attention Mechanism has become the most important mechanism in Deep Learning. It is powers the various langugage models that we hear so often about. It also has various applications such as in Vision tasks.\n",
    "1. The whole idea of Attention can actually be quite confusing. Thankfully, the implementation of Attention (in Numpy) is not. It requires very basic knowledge of Linear Algebra and its forward feed function. \n",
    "1. We will thus discuss the mechanics, and the whole idea of Attention will be left to you to discover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the attention mechanism looks like: \n",
    "\n",
    "$\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "Q, K, V are matrices. To get them, you have an input, X, and then you apply the linear weights on it: \n",
    "\n",
    "$Q = XW_q$  \n",
    "$K = XW_K$  \n",
    "$V = XW_V$  \n",
    "\n",
    "$X$ can be your original input or it could be the output from a previous layer. $W_q$, $W_k$, $W_v$ are weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Create a function that will be used to create $X$, $W_q$, $W_k$, $W_v$\n",
    "\n",
    "1. Use the numpy's random.randn to generate the data and weights. Recall that it takes in the dimension that you want eg randn(2,3) is a 2x3 dimension array. \n",
    "1. We will stick to only 2 dimensions for each of the weights. Example, X would be $\\mathbb{R}^{2\\times32}$\n",
    "1. Ensure that the X and Weights are conformable. Hence, if $X \\in \\mathbb{R}^{2\\times32}$, then $W_q \\in \\mathbb{R}^{32\\times D}$.\n",
    "\n",
    "<details>\n",
    "<summary>Click here to reveal answer</summary>\n",
    "\n",
    "```\n",
    "# initialise X, q_weights, k_weights, v_weights\n",
    "def generate_data_and_weights(num_data: int, num_features: int, num_out_dims: int) -> tuple(np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        num_data (int): number of rows in X\n",
    "        num_features (int): number of columns/features in X\n",
    "        num_out_dims (int): output dimension of q, k and v weights\n",
    "\n",
    "    Returns:\n",
    "        tuple(np.ndarray): X, q_weights, k_weights and v_weights\n",
    "    \"\"\"\n",
    "    X = np.random.randn(num_data,num_features)\n",
    "    q_weights = np.random.randn(num_features, num_out_dims)\n",
    "    k_weights = np.random.randn(num_features, num_out_dims)\n",
    "    v_weights = np.random.randn(num_features, num_out_dims)\n",
    "    return X, q_weights, k_weights, v_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise X, q_weights, k_weights, v_weights\n",
    "def generate_data_and_weights(num_data: int, num_features: int, num_out_dims: int) -> tuple(np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        num_data (int): number of rows in X\n",
    "        num_features (int): number of columns/features in X\n",
    "        num_out_dims (int): output dimension of q, k and v weights\n",
    "\n",
    "    Returns:\n",
    "        tuple(np.ndarray): X, q_weights, k_weights and v_weights\n",
    "    \"\"\"\n",
    "\n",
    "    X = None\n",
    "    q_weights = None\n",
    "    k_weights = None\n",
    "    v_weights = None\n",
    "    return X, q_weights, k_weights, v_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, q_weights, k_weights, v_weights = generate_data_and_weights(196, 768, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: \n",
    "\n",
    "Create a function that takes in X, q_weights, k_weights and v_weights, and returns Q, K, V, where the Q, K, V are the linear encoding of X and the respective weights\n",
    "\n",
    "<details>\n",
    "<summary>Click here to reveal answer</summary>\n",
    "\n",
    "```\n",
    "def get_QKV(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray) -> tuple(np.ndarray):\n",
    "    Q = X@q_weights\n",
    "    K = X@k_weights\n",
    "    V = X@v_weights\n",
    "    return Q, K, V\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_QKV(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray) -> tuple(np.ndarray):\n",
    "    Q = None\n",
    "    K = None\n",
    "    V = None\n",
    "    return Q, K, V\n",
    "\n",
    "Q, K, V = get_QKV(X, q_weights, k_weights, v_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have obtained the Q, K and V, you want work on the interaction between Q and K, and normalise it by the dimension of K. \n",
    "\n",
    "$A = \\frac{Q K^T}{\\sqrt{d_k}}$\n",
    "\n",
    "Task:\n",
    "\n",
    "Create a function that takes Q, K and D, and return A, which is the normalised interaction between Q and K\n",
    "\n",
    "The output should be an NxN matrix, where is the number of rows that X has. \n",
    "\n",
    "<details>\n",
    "<summary>Click here to reveal answer</summary>\n",
    "\n",
    "```\n",
    "def get_attention(Q: np.ndarray, K: np.ndarray, D: int) -> np.ndarray:\n",
    "    A = Q@K.T / np.sqrt(D)\n",
    "    return A\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_attention(Q: np.ndarray, K: np.ndarray, D: int) -> np.ndarray:\n",
    "    A = None\n",
    "    return A\n",
    "\n",
    "A = get_attention(Q, K, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is to softmax the attention output along the columns. Due to complexities revolving handling exponents and logarithms, we will instead use the scipy's softmax function `softmax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = softmax(A, axis = -2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the actual attention, we want calculate the self-attention. Practically, it simply means doing a matmul of A and V. \n",
    "\n",
    "Task: complete the function `get_self_attention` below which takes in the A and V, performs matmul return SA.  \n",
    "\n",
    "<details>\n",
    "<summary>Click here for the answer</summary>\n",
    "```\n",
    "def get_self_attention(A, V):\n",
    "    SA = A@V\n",
    "    return SA\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_attention(A, V):\n",
    "    SA = None\n",
    "    return SA\n",
    "SA = get_self_attention(A, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Put together the functions as defined above as a single function: `single_head_self_attention`. It should take in `X, q_weights, k_weights, v_weights, D` as the parameters, and return the self-attention scores, `SA`\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the answer</summary>\n",
    "\n",
    "```\n",
    "def single_head_self_attention(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray, D: int) -> np.ndarray:\n",
    "    Q, K, V = get_QKV(X, q_weights, k_weights, v_weights)\n",
    "    A = get_attention(Q, K, D)\n",
    "    A = softmax(A, axis = -2)\n",
    "    SA = get_self_attention(A, V)\n",
    "    return SA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_self_attention(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray, D: int) -> np.ndarray:\n",
    "    SA = None\n",
    "    return SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Another concept that was introduced in the paper `Attention is all you need` is the  Multi-Head Attention. \n",
    "\n",
    "![](./msa-paper.png)\n",
    "\n",
    "The original diagram above raises more questions than answers:\n",
    "\n",
    "1. Is the data first split, then run through the heads, before concatenating, or is the data is multiplied to on W -> split to the different heads -> calculate self-attention -> concatenating. \n",
    "1. Another minor question is whether each head is the size of the data itself. Eg above example if data has 32 features, do the individual heads hence have to accomodate 32 features each?  \n",
    "\n",
    "We will use the implementation as guided by PyTorch's own documentation - https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention\n",
    "\n",
    "```\n",
    "num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
    "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
    "```\n",
    "\n",
    "Interpretation:\n",
    "1. We decide the embedding dim. In our context, the dim should be 8. \n",
    "1. The number of head is 2. Hence, each head will only have dimension of 4\n",
    "\n",
    "How we might intuitively do this is:\n",
    "\n",
    "1. You have X and the Weights. Let's say X is n x 32, weights are 32x8. \n",
    "1. We want to split by heads. Heads here is 2\n",
    "1. Split X by the number of heads: n x 2 x 16\n",
    "1. Split the weights as well: 2 x 16 x 4 \n",
    "1. Do matmul to get n x 2 x 4\n",
    "\n",
    "That might work. However, it requires you to loop over each head, which is inefficient. Instead, let's flip it another way:\n",
    "\n",
    "1. Do mat mul on the X and weights -> n x 32 . 32 x 8 -> n x 8\n",
    "1. reshape the output to 2 heads -> n x 2 x 4\n",
    "1. Perform the matmul along the head axis (n remains untouched) Here, use `einsum` to control the dimensions\n",
    "1. Use the same concept to get the softmax and the self-attention. \n",
    "1. reshape back \n",
    "\n",
    "Task: Create the `multi_head_attention`, with the `X, q_weights, k_weights, v_weights, D, num_heads` parameters\n",
    "\n",
    "<details>\n",
    "<summary>Click here to reveal the answers</summary>\n",
    "\n",
    "```\n",
    "def multi_head_attention(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray, D: int, h: int) -> np.ndarray:\n",
    "    dims = (X.shape[0], h, D//h)\n",
    "    Q, K, V = get_QKV(X, q_weights, k_weights, v_weights)\n",
    "\n",
    "    Q = Q.reshape(dims)\n",
    "    K = K.reshape(dims)\n",
    "    V = V.reshape(dims)\n",
    "\n",
    "    def get_multi_head_attention(Q, K, D):\n",
    "        # return Q@np.transpose(K, (0, 2, 1)) / np.sqrt(D)\n",
    "        return np.einsum('ijk,ikl->ijl', Q, K.transpose((0,2,1))) / np.sqrt(D)\n",
    "     \n",
    "    A = get_multi_head_attention(Q, K, D)\n",
    "    A = softmax(A, axis = -2)\n",
    "\n",
    "    SA = A@V\n",
    "\n",
    "    # reshape back to original shape\n",
    "    SA = SA.reshape(X.shape[0], D)\n",
    "    return SA\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray, D: int, h: int) -> np.ndarray:\n",
    "    \n",
    "    SA = None\n",
    "    \n",
    "    return SA\n",
    "\n",
    "MSA = multi_head_attention(X, q_weights, k_weights, v_weights, 8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X, q_weights, k_weights, v_weights = generate_data_and_weights(196, 768, 8)\n",
    "SA = single_head_self_attention(X, q_weights, k_weights, v_weights, 8)\n",
    "MSA = multi_head_attention(X, q_weights, k_weights, v_weights, 8, 2)\n",
    "\n",
    "print (SA.shape)\n",
    "print (MSA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click Here for the full answers</summary>\n",
    "\n",
    "```\n",
    "def generate_data_and_weights(num_data: int, num_features: int, num_out_dims: int) -> tuple(np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        num_data (int): number of rows in X\n",
    "        num_features (int): number of columns/features in X\n",
    "        num_out_dims (int): output dimension of q, k and v weights\n",
    "\n",
    "    Returns:\n",
    "        tuple(np.ndarray): X, q_weights, k_weights and v_weights\n",
    "    \"\"\"\n",
    "    X = np.random.randn(num_data, num_features)\n",
    "    q_weights = np.random.randn(num_features, num_out_dims)\n",
    "    k_weights = np.random.randn(num_features, num_out_dims)\n",
    "    v_weights = np.random.randn(num_features, num_out_dims)\n",
    "    return X, q_weights, k_weights, v_weights\n",
    "\n",
    "def get_QKV(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray) -> tuple(np.ndarray):\n",
    "    Q = X@q_weights\n",
    "    K = X@k_weights\n",
    "    V = X@v_weights\n",
    "    return Q, K, V\n",
    "\n",
    "def get_attention(Q: np.ndarray, K: np.ndarray, D: int) -> np.ndarray:\n",
    "    A = Q@K.T / np.sqrt(D)\n",
    "    return A\n",
    "\n",
    "def get_self_attention(A, V):\n",
    "    SA = A@V\n",
    "    return SA\n",
    "\n",
    "def single_head_self_attention(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray, D: int) -> np.ndarray:\n",
    "    Q, K, V = get_QKV(X, q_weights, k_weights, v_weights)\n",
    "    A = get_attention(Q, K, D)\n",
    "    A = softmax(A, axis = -2)\n",
    "    SA = get_self_attention(A, V)\n",
    "    return SA\n",
    "\n",
    "def multi_head_attention(X: np.ndarray, q_weights: np.ndarray, k_weights: np.ndarray, v_weights: np.ndarray, D: int, h: int) -> np.ndarray:\n",
    "    dims = (X.shape[0], h, D//h)\n",
    "    Q, K, V = get_QKV(X, q_weights, k_weights, v_weights)\n",
    "\n",
    "    Q = Q.reshape(dims)\n",
    "    K = K.reshape(dims)\n",
    "    V = V.reshape(dims)\n",
    "\n",
    "    def get_multi_head_attention(Q, K, D):\n",
    "        # return Q@np.transpose(K, (0, 2, 1)) / np.sqrt(D)\n",
    "        return np.einsum('ijk,ikl->ijl', Q, K.transpose((0,2,1))) / np.sqrt(D)\n",
    "     \n",
    "    A = get_multi_head_attention(Q, K, D)\n",
    "    A = softmax(A, axis = -2)\n",
    "\n",
    "    SA = A@V\n",
    "\n",
    "    # reshape back to original shape\n",
    "    SA = SA.reshape(X.shape[0], D)\n",
    "    return SA\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
