{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neoce\\miniconda3\\envs\\pytorch_cpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "import math\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Mechanism\n",
    "\n",
    "$ \\alpha_i=\\frac{\\exp \\left(f_{\\text {attn }}\\left(\\text { key }_i, \\text { query }\\right)\\right)}{\\sum_j \\exp \\left(f_{\\text {attn }}\\left(\\text { key }_j, \\text { query }\\right)\\right)}, \\quad \\text { out }=\\sum_i \\alpha_i \\cdot \\text { value }_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Function: Scaled Products\n",
    "\n",
    "Implement the Scaled Dot Product. The mechanism of Scaled Dot Product is as follows:\n",
    "\n",
    "We have a set of queries, keys and values:\n",
    "\n",
    "Queries: $Q \\in \\mathbb{R}^{T \\times d_k}$\n",
    "Keys: $K \\in \\mathbb{R}^{T \\times d_k}$\n",
    "Values: $V \\in \\mathbb{R}^{T \\times d_v}$\n",
    "\n",
    "where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively\n",
    "\n",
    "Our goal here are:\n",
    "\n",
    "1) measure the similarity between Q and K. We achieve this by doing a dot product between Q and K. This we can say is $QK^\\top \\in \\mathbb R^{T \\times T}$ Each row in $QK^\\top$ is the attention logits for a specific element $i$ to all other elements in the sequence.\n",
    "1) Scale $QK^\\top$ by $\\frac{1}{\\sqrt{d_k}}$. This will ensure that the variance is scaled down to ~ $\\sigma^2$\n",
    "1) Apply mask fill. If there are masks involved, then change those masks to some arbitrarily small number. We will use binary masks, where 0 means you apply the mask fill, and 1 means you don't and should be attended to. Use [masked_fill_](https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_). Note that the mask fill is applied to the attention logits, which is of the dimension $R^{T \\times T}$\n",
    "1) Apply softmax function to $\\frac{Q K^T}{\\sqrt{d_k}}$ to get **attentions**\n",
    "1) Multiply $\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)$ by $V$ to get **attention values**\n",
    "\n",
    "Return the **attention values** and **attentions**, respectively.\n",
    "\n",
    "Very important:\n",
    "\n",
    "Make sure to account for batch dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q: torch.tensor, k: torch.tensor, v: torch.tensor, mask: torch.tensor = None) -> Tuple[torch.tensor, torch.tensor]: \n",
    "    \"\"\"implements the scaled dot products\n",
    "\n",
    "    Args:\n",
    "        q (torch.tensor): _description_\n",
    "        k (torch.tensor): _description_\n",
    "        v (torch.tensor): _description_\n",
    "        mask (torch.tensor, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.tensor, torch.tensor]: _description_\n",
    "    \"\"\"\n",
    "    # step 1: peform Q@K.T\n",
    "    # be sure to do for batch dimension\n",
    "\n",
    "    attention_logits = None\n",
    "\n",
    "    # step 2: scale QK.T\n",
    "\n",
    "\n",
    "    # step 3: apply mask fill. Fill with -9e15\n",
    "\n",
    "\n",
    "    # step 4: softmax function on your logits to get your attention\n",
    "    \n",
    "    attention = None\n",
    "\n",
    "    # step 5: do a matmul with V to get your values\n",
    "    \n",
    "    values = None\n",
    "\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my implementation\n",
    "\n",
    "def scaled_dot_product(q: torch.tensor, k: torch.tensor, v: torch.tensor, mask: torch.tensor = None) -> Tuple[torch.tensor, torch.tensor]: \n",
    "    \"\"\"implements the scaled dot products\n",
    "\n",
    "    Args:\n",
    "        q (torch.tensor): _description_\n",
    "        k (torch.tensor): _description_\n",
    "        v (torch.tensor): _description_\n",
    "        mask (torch.tensor, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.tensor, torch.tensor]: _description_\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # replace values and attention variables\n",
    "\n",
    "    # step 1: peform Q@K.T\n",
    "    # be sure to do for batch dimension\n",
    "\n",
    "    qk = q@k.transpose(-2, -1)\n",
    "\n",
    "    # step 2: scale QK.T\n",
    "\n",
    "    d_k = q.shape[-1]\n",
    "\n",
    "    qk_scaled = qk / (math.sqrt(d_k))\n",
    "\n",
    "    # step 3: apply mask fill. Fill with -9e15\n",
    "\n",
    "    if mask is not None:\n",
    "        qk_scaled = qk_scaled.masked_fill(mask == 0, -9e15)\n",
    "\n",
    "    # step 4: softmax function on your logits to get your attention\n",
    "    \n",
    "    attention = F.softmax(qk_scaled, dim = -1)\n",
    "\n",
    "    # step 5: do a matmul with V to get your values\n",
    "\n",
    "    values = attention@v\n",
    "\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the standard implementation\n",
    "\n",
    "def scaled_dot_product_test(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "batch, seq_len, d_k = 3, 3, 2\n",
    "\n",
    "# test only one batch\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "val, att = scaled_dot_product_test(q,k,v)\n",
    "torch.allclose(val, values)\n",
    "torch.allclose(att, attention)\n",
    "# print(\"Q\\n\", q)\n",
    "# print(\"K\\n\", k)\n",
    "# print(\"V\\n\", v)\n",
    "# print(\"Values\\n\", values)\n",
    "# print(\"Attention\\n\", attention)\n",
    "\n",
    "# test multiple batches\n",
    "q = torch.randn(batch, seq_len, d_k)\n",
    "k = torch.randn(batch, seq_len, d_k)\n",
    "v = torch.randn(batch, seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "# print(\"Q\\n\", q)\n",
    "# print(\"K\\n\", k)\n",
    "# print(\"V\\n\", v)\n",
    "# print(\"Values\\n\", values)\n",
    "# print(\"Attention\\n\", attention)\n",
    "\n",
    "val, att = scaled_dot_product_test(q,k,v)\n",
    "torch.allclose(val, values)\n",
    "torch.allclose(att, attention)\n",
    "\n",
    "# test that masking works right\n",
    "\n",
    "mask = torch.eye(seq_len)\n",
    "values, attention = scaled_dot_product(q, k, v, mask)\n",
    "val, att = scaled_dot_product_test(q,k,v, mask)\n",
    "torch.allclose(val, values)\n",
    "torch.allclose(att, attention)\n",
    "\n",
    "print (\"Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what is going on with the Scaled Dot Product Attention, we turn our attention (hah!) to the Multi-Head Attention Network. In a way, we are giving head to our data (woah!).\n",
    "\n",
    "Let's leave the `Multi` part aside and talk about what giving head to our data means. A head contains the Q K and V weights. Using those weights we then turn our sequence of inputs into a matrix where each word (element) our sequence is a vector. We then take these vectors, do the linear combination and get Q K and V outputs. We then run the Scaled Dot Product to get the Z. \n",
    "\n",
    "So instead of just have one head, we are giving our data mulitple heads! In the original paper, there were 8 heads, like a Hydra. The unique this is that each of these heads' weights are totally different in their initialisation. However, within each head, the Q K and V weights should be initialised the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to support different mask shapes.\n",
    "# Output shape supports (batch_size, number of heads, seq length, seq length)\n",
    "# If 2D: broadcasted over batch size and number of heads\n",
    "# If 3D: broadcasted over number of heads\n",
    "# If 4D: leave as is\n",
    "def expand_mask(mask):\n",
    "    assert mask.ndim > 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.unsqueeze(1)\n",
    "    while mask.ndim < 4:\n",
    "        mask = mask.unsqueeze(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
